{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032f23e654f5483489ce18ea338550a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import FileUpload\n",
    "\n",
    "upload_widget = FileUpload()\n",
    "display(upload_widget)\n",
    "\n",
    "# Once a file is uploaded, you can access it via:\n",
    "uploaded_file = upload_widget.value\n",
    "print(uploaded_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "video_path = \"C:\\\\Users\\\\gunda\\\\OneDrive\\\\Desktop\\\\Videosummarizer\\\\myenv\\\\video2.mp4\" \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file. Check the file path!\")\n",
    "else:\n",
    "    print(\"Video loaded successfully!\")\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 453 frames and saved in 'frames' folder.\n"
     ]
    }
   ],
   "source": [
    "# Extract Frames\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = 0\n",
    "frame_skip = 10  # Extract every 10th frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_count % frame_skip == 0:\n",
    "        frame_filename = f\"frames/frame_{frame_count}.jpg\"\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "    frame_count += 1\n",
    "cap.release()\n",
    "print(f\"Extracted {frame_count//frame_skip} frames and saved in 'frames' folder.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyframes saved: 334\n"
     ]
    }
   ],
   "source": [
    "# Extract Keyframes based on Scene Change\n",
    "os.makedirs(\"keyframes\", exist_ok=True)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read first frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "frame_count = 0\n",
    "keyframe_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    diff = cv2.absdiff(prev_gray, gray)\n",
    "    diff_score = np.sum(diff) / (diff.shape[0] * diff.shape[1])\n",
    "    if diff_score > 10:\n",
    "        keyframe_count += 1\n",
    "        keyframe_filename = f\"keyframes/keyframe_{keyframe_count}.jpg\"\n",
    "        cv2.imwrite(keyframe_filename, frame)\n",
    "        prev_gray = gray\n",
    "cap.release()\n",
    "print(f\"Total keyframes saved: {keyframe_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\gunda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 10 keyframes and saved in 'selected_keyframes'\n"
     ]
    }
   ],
   "source": [
    "# Clustering Keyframes\n",
    "keyframes_dir = \"keyframes\"\n",
    "keyframes = []\n",
    "keyframe_paths = sorted([os.path.join(keyframes_dir, f) for f in os.listdir(keyframes_dir)])\n",
    "for path in keyframe_paths:\n",
    "    img = cv2.imread(path)\n",
    "    if img is not None:\n",
    "        keyframes.append(img)\n",
    "features = []\n",
    "for img in keyframes:\n",
    "    hist = cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    features.append(hist.flatten())\n",
    "features = np.array(features)\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(features)\n",
    "selected_keyframes = []\n",
    "for cluster in range(num_clusters):\n",
    "    indices = np.where(labels == cluster)[0]\n",
    "    representative_idx = indices[0]\n",
    "    selected_keyframes.append(keyframes[representative_idx])\n",
    "output_dir = \"selected_keyframes\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for i, img in enumerate(selected_keyframes):\n",
    "    cv2.imwrite(f\"{output_dir}/keyframe_{i}.jpg\", img)\n",
    "print(f\"Selected {num_clusters} keyframes and saved in '{output_dir}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c50e47e2c24ee4ad07568d72f5aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gunda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gunda\\.cache\\huggingface\\hub\\models--Salesforce--blip-image-captioning-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145cfdddb1f642b39d175748af56dcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c3960c456f4a3d8f9cdc304742a6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83195061513848858f24a2cf5df56e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb364dfa8a54d2989ad326f1ca6c38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fc6592ae9f45d68069487c83815aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f87a6f33ad441b7bce7d4ebaa0b259c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b3fda904de4b14b249c1afbd88fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption for selected_keyframes\\keyframe_0.jpg: a screen shot of a map with a red marker\n",
      "Caption for selected_keyframes\\keyframe_1.jpg: a person holding a phone with a map on it\n",
      "Caption for selected_keyframes\\keyframe_2.jpg: a screen shot of a man and woman riding bikes\n",
      "Caption for selected_keyframes\\keyframe_3.jpg: a website with a picture of people riding bikes\n",
      "Caption for selected_keyframes\\keyframe_4.jpg: a screen shot of a person riding a bike\n",
      "Caption for selected_keyframes\\keyframe_5.jpg: a person holding a phone with a map on it\n",
      "Caption for selected_keyframes\\keyframe_6.jpg: a screen shot of a website with a map and a person on a bike\n",
      "Caption for selected_keyframes\\keyframe_7.jpg: a website with a picture of a person riding a bike\n",
      "Caption for selected_keyframes\\keyframe_8.jpg: a screenshote website with a map and location\n",
      "Caption for selected_keyframes\\keyframe_9.jpg: cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle cycle\n"
     ]
    }
   ],
   "source": [
    "# Generate Captions\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "captions = {}\n",
    "keyframe_paths = sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir)])\n",
    "for path in keyframe_paths:\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    captions[path] = caption\n",
    "    print(f\"Caption for {path}: {caption}\")\n",
    "with open(\"captions.txt\", \"w\") as f:\n",
    "    for key, value in captions.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Video Summary Generated:\n",
      "Video captures a sequence of events showcasing urban cycling and navigation. A screen shot of a map with a red marker. A person holding a phone with a map on it. A website with a picture of people riding bikes.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Read captions from file\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Function to clean and structure captions\n",
    "def clean_caption(caption):\n",
    "    caption = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', caption)  # Remove repeated words\n",
    "    caption = caption.capitalize().strip()\n",
    "    return caption\n",
    "\n",
    "# Convert captions into a structured paragraph\n",
    "story_text = \"The video captures a sequence of events showcasing urban cycling and navigation. \"  # Introduction\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.split(\":\")\n",
    "    if len(parts) > 1:\n",
    "        caption = clean_caption(parts[1].strip())\n",
    "        story_text += f\" {caption}. \"\n",
    "\n",
    "# Generate a better summary\n",
    "summary = summarizer(story_text, max_length=80, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "# Save summary to file\n",
    "with open(\"video_summary.txt\", \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"✅ Video Summary Generated:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
